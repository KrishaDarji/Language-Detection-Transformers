{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Dependencies\n",
    "\n",
    "This section imports all the necessary libraries and modules required for the project:\n",
    "\n",
    "- **Core libraries**: `numpy`, `pandas`, `os`, `sys` for data manipulation and system interactions.\n",
    "- **Path and progress utilities**: `Path` from `pathlib` and `tqdm` for handling paths and progress bars.\n",
    "- **Audio processing**: `torchaudio` and `librosa` for audio loading and transformations.\n",
    "- **IPython display**: `ipd` for audio playback in the notebook.\n",
    "- **Data handling**: `datasets` for managing and loading datasets with caching enabled.\n",
    "- **Machine learning tools**: `scikit-learn` for splitting data into training and testing sets.\n",
    "- **Deep learning frameworks**:\n",
    "  - `torch` and `torch.nn` for creating and managing neural networks.\n",
    "  - Transformers library for leveraging the Wav2Vec2 processor and feature extractor.\n",
    "- **Modeling utilities**:\n",
    "  - Data class structures for model outputs.\n",
    "  - Training-related components like `Trainer` and `TrainingArguments`.\n",
    "  - Loss functions like `CrossEntropyLoss` for classification tasks.\n",
    "- **Version checks**:\n",
    "  - Ensures compatibility with `torch` versions and integrates with NVIDIA's AMP for mixed-precision training.\n",
    "\n",
    "These imports lay the groundwork for the project, enabling the integration of Wav2Vec2 for language identification from audio files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:10.319412Z",
     "iopub.status.busy": "2024-04-27T07:20:10.319031Z",
     "iopub.status.idle": "2024-04-27T07:20:10.324623Z",
     "shell.execute_reply": "2024-04-27T07:20:10.323725Z",
     "shell.execute_reply.started": "2024-04-27T07:20:10.319382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "from transformers import AutoConfig, Wav2Vec2Processor, Wav2Vec2FeatureExtractor \n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from datasets import load_dataset\n",
    "from datasets import set_caching_enabled\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import EvalPrediction\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import torch.nn as nn\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from packaging import version\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection and Organization\n",
    "\n",
    "In this section, we organize the dataset for language identification:\n",
    "\n",
    "1. **Dataset Directory**:\n",
    "   - The variable `data_dir` points to the directory containing audio files. Here, the dataset is expected to be organized with subdirectories named after the language labels.\n",
    "\n",
    "2. **Data Structure**:\n",
    "   - A list named `data` is initialized to store metadata for each audio file, including:\n",
    "     - `path`: Full file path to the `.wav` audio file.\n",
    "     - `label`: The language label derived from the subdirectory name.\n",
    "\n",
    "3. **Iterating Through Files**:\n",
    "   - `os.walk()` recursively traverses the `data_dir` directory.\n",
    "   - For each `.wav` file encountered:\n",
    "     - The `path` is constructed using `os.path.join`.\n",
    "     - The `label` is extracted from the parent directory name.\n",
    "     - These details are stored as a dictionary in the `data` list.\n",
    "\n",
    "4. **Progress Tracking**:\n",
    "   - `tqdm` provides a progress bar to visualize the traversal of the directory.\n",
    "\n",
    "This process prepares a structured dataset with audio file paths and their respective language labels, which will be used for further processing and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:10.326935Z",
     "iopub.status.busy": "2024-04-27T07:20:10.326504Z",
     "iopub.status.idle": "2024-04-27T07:20:52.547653Z",
     "shell.execute_reply": "2024-04-27T07:20:52.546780Z",
     "shell.execute_reply.started": "2024-04-27T07:20:10.326903Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:42,  4.69s/it]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/kaggle/input/languages\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(data_dir)):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):  \n",
    "            path = os.path.join(root, file)\n",
    "            name = os.path.splitext(file)[0]  \n",
    "            label = os.path.basename(root)\n",
    "            data.append({\n",
    "                \"path\": path,\n",
    "                \"label\": label\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Previewing the Dataset\n",
    "\n",
    "1. **Creating a DataFrame**:\n",
    "   - The collected metadata is converted into a Pandas DataFrame using `pd.DataFrame(data)`. This provides a tabular structure, making it easier to manage and analyze the dataset.\n",
    "\n",
    "2. **Previewing the Data**:\n",
    "   - `df.head()` displays the first five rows of the DataFrame, allowing for a quick inspection of its structure and content. The expected columns are:\n",
    "     - `path`: File path of the audio sample.\n",
    "     - `label`: Language label associated with the audio sample.\n",
    "\n",
    "This step ensures that the dataset is correctly structured and ready for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:52.549307Z",
     "iopub.status.busy": "2024-04-27T07:20:52.549035Z",
     "iopub.status.idle": "2024-04-27T07:20:52.596002Z",
     "shell.execute_reply": "2024-04-27T07:20:52.595103Z",
     "shell.execute_reply.started": "2024-04-27T07:20:52.549284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/languages/japanese/japanese_3123...</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/languages/japanese/japanese_3220...</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/languages/japanese/japanese_2391...</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/languages/japanese/japanese_3613...</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/languages/japanese/japanese_2229...</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path     label\n",
       "0  /kaggle/input/languages/japanese/japanese_3123...  japanese\n",
       "1  /kaggle/input/languages/japanese/japanese_3220...  japanese\n",
       "2  /kaggle/input/languages/japanese/japanese_2391...  japanese\n",
       "3  /kaggle/input/languages/japanese/japanese_3613...  japanese\n",
       "4  /kaggle/input/languages/japanese/japanese_2229...  japanese"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Label Inspection and Distribution Analysis\n",
    "\n",
    "1. **Listing Unique Labels**:\n",
    "   - `df[\"label\"].unique()` identifies and displays the unique language labels present in the dataset. This helps verify the categories included for language identification.\n",
    "\n",
    "2. **Analyzing Label Distribution**:\n",
    "   - `df.groupby(\"label\").count()[[\"path\"]]` calculates the count of audio files for each label (language).\n",
    "   - This step provides insights into the dataset balance, helping to identify any underrepresented or overrepresented languages.\n",
    "\n",
    "Understanding the label distribution is crucial for designing a robust and fair training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:52.597916Z",
     "iopub.status.busy": "2024-04-27T07:20:52.597632Z",
     "iopub.status.idle": "2024-04-27T07:20:52.629425Z",
     "shell.execute_reply": "2024-04-27T07:20:52.628622Z",
     "shell.execute_reply.started": "2024-04-27T07:20:52.597892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['japanese' 'hindi' 'gujarati' 'russian' 'german' 'sanskrit' 'italian'\n",
      " 'spanish']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>german</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gujarati</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hindi</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japanese</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russian</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanskrit</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          path\n",
       "label         \n",
       "german    5000\n",
       "gujarati  5000\n",
       "hindi     5000\n",
       "italian   5000\n",
       "japanese  5000\n",
       "russian   5000\n",
       "sanskrit  5000\n",
       "spanish   5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Labels: \", df[\"label\"].unique())\n",
    "print()\n",
    "df.groupby(\"label\").count()[[\"path\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Training and Testing Sets\n",
    "\n",
    "1. **Defining the Save Path**:\n",
    "   - `save_path` specifies the directory where the train and test DataFrames will be saved as `.csv` files.\n",
    "\n",
    "2. **Splitting the Dataset**:\n",
    "   - `train_test_split()` splits the DataFrame into training and testing sets:\n",
    "     - `test_size=0.2` allocates 20% of the data for testing.\n",
    "     - `stratify=df[\"label\"]` ensures that the label distribution is maintained in both splits.\n",
    "   - `random_state=4` ensures reproducibility of the split.\n",
    "\n",
    "3. **Resetting Indexes**:\n",
    "   - The `reset_index(drop=True)` method resets the indices of the training and testing DataFrames, making them continuous and independent of the original DataFrame.\n",
    "\n",
    "4. **Saving the Splits**:\n",
    "   - Both DataFrames are saved as tab-separated `.csv` files in the specified directory:\n",
    "     - `train.csv`: Contains training data.\n",
    "     - `test.csv`: Contains testing data.\n",
    "\n",
    "5. **Shape Verification**:\n",
    "   - The dimensions of `train_df` and `test_df` are printed to confirm the split and verify the number of samples in each set.\n",
    "\n",
    "This step prepares the training and testing datasets for model training and evaluation, ensuring an organized workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:52.630657Z",
     "iopub.status.busy": "2024-04-27T07:20:52.630424Z",
     "iopub.status.idle": "2024-04-27T07:20:52.834831Z",
     "shell.execute_reply": "2024-04-27T07:20:52.833959Z",
     "shell.execute_reply.started": "2024-04-27T07:20:52.630637Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 2)\n",
      "(8000, 2)\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/kaggle/working/\"\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=4, stratify=df[\"label\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset into Hugging Face's `datasets` Library\n",
    "\n",
    "1. **Specifying Data Files**:\n",
    "   - A dictionary `data_files` is created to specify the paths of the training and validation `.csv` files:\n",
    "     - `\"train\"`: Path to the training dataset.\n",
    "     - `\"validation\"`: Path to the testing dataset.\n",
    "\n",
    "2. **Loading the Dataset**:\n",
    "   - `load_dataset()` reads the `.csv` files into a `datasets.DatasetDict` object:\n",
    "     - The `delimiter=\"\\t\"` parameter ensures that the tab-separated format of the files is correctly parsed.\n",
    "   - The loaded datasets are split into:\n",
    "     - `train_dataset`: Training dataset.\n",
    "     - `eval_dataset`: Validation (testing) dataset.\n",
    "\n",
    "3. **Dataset Summary**:\n",
    "   - `print(train_dataset)` and `print(eval_dataset)` provide a summary of the training and validation datasets, including the number of samples and the column structure.\n",
    "\n",
    "This step integrates the preprocessed data with the Hugging Face `datasets` library, enabling streamlined data manipulation and compatibility with the training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:52.836280Z",
     "iopub.status.busy": "2024-04-27T07:20:52.836007Z",
     "iopub.status.idle": "2024-04-27T07:20:53.357183Z",
     "shell.execute_reply": "2024-04-27T07:20:53.356239Z",
     "shell.execute_reply.started": "2024-04-27T07:20:52.836256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2342a784cc342fe867800e225bd3ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c281cd1dfd46e786ece166b9b72eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['path', 'label'],\n",
      "    num_rows: 32000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['path', 'label'],\n",
      "    num_rows: 8000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": \"/kaggle/working/train.csv\", \n",
    "    \"validation\": \"/kaggle/working/test.csv\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset Columns and Labels for Model Training\n",
    "\n",
    "1. **Defining Input and Output Columns**:\n",
    "   - `input_column`: Specifies the column containing the paths to audio files (features) for model input.\n",
    "   - `output_column`: Specifies the column containing the language labels (targets) for model training.\n",
    "\n",
    "2. **Extracting Unique Labels**:\n",
    "   - `train_dataset.unique(output_column)` retrieves all unique language labels from the training dataset.\n",
    "   - `label_list.sort()` ensures the labels are sorted in ascending order for consistency.\n",
    "\n",
    "3. **Counting Labels**:\n",
    "   - `num_labels` calculates the total number of unique labels, which is used to configure the model's output layer for classification.\n",
    "\n",
    "This step organizes the dataset's structure and prepares label-related metadata required for model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:55.027255Z",
     "iopub.status.busy": "2024-04-27T07:20:55.026689Z",
     "iopub.status.idle": "2024-04-27T07:20:55.039686Z",
     "shell.execute_reply": "2024-04-27T07:20:55.038768Z",
     "shell.execute_reply.started": "2024-04-27T07:20:55.027227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_column = \"path\"\n",
    "output_column = \"label\"\n",
    "label_list = train_dataset.unique(output_column)\n",
    "label_list.sort()  \n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Wav2Vec2 Model and Pooling Strategy\n",
    "\n",
    "1. **Model Name or Path**:\n",
    "   - `model_name_or_path` specifies the pre-trained Wav2Vec2 model to be used:\n",
    "     - `\"facebook/wav2vec2-base-100k-voxpopuli\"` is a Wav2Vec2 model trained on the VoxPopuli dataset, tailored for speech processing tasks.\n",
    "\n",
    "2. **Pooling Mode**:\n",
    "   - `pooling_mode` determines how the output embeddings from the Wav2Vec2 model are aggregated:\n",
    "     - `\"mean\"`: The embeddings are averaged across the sequence length, resulting in a fixed-size representation.\n",
    "\n",
    "These configurations define the backbone model and feature aggregation method for the language identification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:55.044128Z",
     "iopub.status.busy": "2024-04-27T07:20:55.043789Z",
     "iopub.status.idle": "2024-04-27T07:20:55.047892Z",
     "shell.execute_reply": "2024-04-27T07:20:55.047010Z",
     "shell.execute_reply.started": "2024-04-27T07:20:55.044105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"facebook/wav2vec2-base-100k-voxpopuli\"\n",
    "pooling_mode = \"mean\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Model Configuration\n",
    "\n",
    "1. **Loading Pre-trained Model Configuration**:\n",
    "   - `AutoConfig.from_pretrained()` initializes the configuration of the pre-trained Wav2Vec2 model using:\n",
    "     - `model_name_or_path`: The specified Wav2Vec2 model path or identifier.\n",
    "\n",
    "2. **Customizing Configuration for Classification**:\n",
    "   - `num_labels`: Sets the number of unique labels in the dataset.\n",
    "   - `label2id`: Maps each label to a unique integer ID.\n",
    "   - `id2label`: Maps each integer ID back to its corresponding label.\n",
    "   - `finetuning_task`: Specifies the fine-tuning task as `\"wav2vec2_clf\"`, indicating this model will be fine-tuned for classification.\n",
    "\n",
    "3. **Adding Pooling Mode**:\n",
    "   - `setattr(config, 'pooling_mode', pooling_mode)`: Dynamically adds the `pooling_mode` attribute to the configuration, defining how the model will aggregate its output embeddings.\n",
    "\n",
    "This step customizes the pre-trained Wav2Vec2 model for the language identification task, ensuring that the configuration aligns with the dataset and task requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:55.049328Z",
     "iopub.status.busy": "2024-04-27T07:20:55.049023Z",
     "iopub.status.idle": "2024-04-27T07:20:55.326840Z",
     "shell.execute_reply": "2024-04-27T07:20:55.325990Z",
     "shell.execute_reply.started": "2024-04-27T07:20:55.049295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c68777b54141d7b1f0689d55744863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id={label: i for i, label in enumerate(label_list)},\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Feature Extractor and Target Sampling Rate\n",
    "\n",
    "1. **Loading the Wav2Vec2 Feature Extractor**:\n",
    "   - `Wav2Vec2FeatureExtractor.from_pretrained()` loads the feature extractor associated with the pre-trained Wav2Vec2 model. This extractor is responsible for preprocessing the audio input, converting it into the appropriate format for the model.\n",
    "\n",
    "2. **Target Sampling Rate**:\n",
    "   - `feature_extractor.sampling_rate` retrieves the target sampling rate of the pre-trained model.\n",
    "   - The target sampling rate represents the frequency at which audio samples are expected to be processed by the model.\n",
    "\n",
    "3. **Printing the Sampling Rate**:\n",
    "   - The target sampling rate is printed to provide insight into the audio preprocessing requirements.\n",
    "\n",
    "This step ensures that the audio data is processed at the correct sampling rate, aligning with the expectations of the pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:55.328712Z",
     "iopub.status.busy": "2024-04-27T07:20:55.328140Z",
     "iopub.status.idle": "2024-04-27T07:20:55.539021Z",
     "shell.execute_reply": "2024-04-27T07:20:55.538179Z",
     "shell.execute_reply.started": "2024-04-27T07:20:55.328678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3559632a3114ef087ad48fd73239d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = feature_extractor.sampling_rate\n",
    "print(f\"The target sampling rate: {target_sampling_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Data Preprocessing Functions\n",
    "\n",
    "1. **`speech_file_to_array_fn()`**:\n",
    "   - This function converts an audio file to a NumPy array:\n",
    "     - `torchaudio.load(path)` loads the audio file from the specified `path` and returns the audio as a tensor (`speech_array`) along with its sampling rate.\n",
    "     - `torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)` creates a resampler to convert the audio's sampling rate to the target sampling rate.\n",
    "     - The audio tensor is then resampled and converted into a NumPy array, which is returned as `speech`.\n",
    "\n",
    "2. **`label_to_id()`**:\n",
    "   - This function maps a language label to its corresponding ID from the `label_list`:\n",
    "     - If the label is found in the list, its index is returned; otherwise, it returns `-1`.\n",
    "\n",
    "3. **`preprocess_function()`**:\n",
    "   - This function applies preprocessing to the dataset:\n",
    "     - **Speech Data**: A list of processed audio arrays is created by applying `speech_file_to_array_fn()` to each audio file path in `examples[input_column]`.\n",
    "     - **Labels**: A list of label IDs is created by applying `label_to_id()` to each language label in `examples[output_column]`.\n",
    "     - The `feature_extractor` is applied to the speech data to extract features, with the sampling rate set to the target value.\n",
    "     - The labels are added to the result as a new key `\"labels\"`, forming the final processed output.\n",
    "\n",
    "This set of functions prepares the dataset by converting audio files to a suitable format and encoding labels for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:55.540869Z",
     "iopub.status.busy": "2024-04-27T07:20:55.540299Z",
     "iopub.status.idle": "2024-04-27T07:20:55.549335Z",
     "shell.execute_reply": "2024-04-27T07:20:55.548515Z",
     "shell.execute_reply.started": "2024-04-27T07:20:55.540833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    speech_array, sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech \n",
    "\n",
    "def label_to_id(label, label_list):\n",
    "\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "\n",
    "    return label \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n",
    "    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n",
    "\n",
    "    result = feature_extractor(speech_list, sampling_rate=target_sampling_rate)\n",
    "    result[\"labels\"] = list(target_list)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting the Dataset Size for Training and Evaluation\n",
    "\n",
    "1. **Defining Maximum Samples**:\n",
    "   - `max_samples = 3200` sets the upper limit for the number of samples to be used from both the training and evaluation datasets. This helps manage memory usage and training time.\n",
    "\n",
    "2. **Selecting a Subset of the Dataset**:\n",
    "   - `train_dataset.select(range(max_samples))` selects the first `max_samples` samples from the training dataset.\n",
    "   - `eval_dataset.select(range(max_samples))` similarly selects the first `max_samples` samples from the evaluation dataset.\n",
    "\n",
    "This step ensures that only a manageable portion of the dataset is used, allowing for more efficient experimentation and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:55.550608Z",
     "iopub.status.busy": "2024-04-27T07:20:55.550339Z",
     "iopub.status.idle": "2024-04-27T07:20:55.566043Z",
     "shell.execute_reply": "2024-04-27T07:20:55.565217Z",
     "shell.execute_reply.started": "2024-04-27T07:20:55.550586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_samples = 3200\n",
    "train_dataset = train_dataset.select(range(max_samples))\n",
    "eval_dataset = eval_dataset.select(range(max_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Preprocessing to the Datasets\n",
    "\n",
    "1. **Preprocessing the Training Dataset**:\n",
    "   - `train_dataset.map(preprocess_function, batch_size=100, batched=True)` applies the `preprocess_function` to the training dataset.\n",
    "     - `batch_size=100` processes the data in batches of 100 examples, improving efficiency.\n",
    "     - `batched=True` ensures that the `preprocess_function` operates on batches of data, rather than individual examples.\n",
    "\n",
    "2. **Preprocessing the Evaluation Dataset**:\n",
    "   - Similarly, `eval_dataset.map(preprocess_function, batch_size=100, batched=True)` applies the same preprocessing to the evaluation dataset.\n",
    "\n",
    "This step preprocesses both datasets, extracting features from the audio files and encoding the labels, making the data ready for model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:20:55.567299Z",
     "iopub.status.busy": "2024-04-27T07:20:55.567026Z",
     "iopub.status.idle": "2024-04-27T07:23:41.444627Z",
     "shell.execute_reply": "2024-04-27T07:23:41.443590Z",
     "shell.execute_reply.started": "2024-04-27T07:20:55.567276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c58c89090c46b58f51c4d54eee63df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 07:20:59.881816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-27 07:20:59.881950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-27 07:21:00.031945: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40504f6ae4ac422e9dd4de469c64d09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Custom Output Class for the Speech Classifier\n",
    "\n",
    "1. **`SpeechClassifierOutput` Class**:\n",
    "   - This class extends `ModelOutput` from the Hugging Face Transformers library to define the structure of the output returned by the speech classification model.\n",
    "   \n",
    "2. **Attributes**:\n",
    "   - `loss`: An optional attribute to store the loss value (used during training).\n",
    "   - `logits`: The model's raw output logits (predictions before applying softmax) for each input, which is crucial for classification tasks.\n",
    "   - `hidden_states`: An optional tuple of hidden states from intermediate layers of the model. These can be useful for understanding the model's internal representation.\n",
    "   - `attentions`: An optional tuple of attention weights from the model's attention layers, useful for analyzing how the model attends to different parts of the input.\n",
    "\n",
    "This custom output class allows for structured and organized handling of model predictions and internal states, facilitating model evaluation and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:23:41.446028Z",
     "iopub.status.busy": "2024-04-27T07:23:41.445741Z",
     "iopub.status.idle": "2024-04-27T07:23:41.452806Z",
     "shell.execute_reply": "2024-04-27T07:23:41.451853Z",
     "shell.execute_reply.started": "2024-04-27T07:23:41.446005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Wav2Vec2 Model for Speech Classification\n",
    "\n",
    "1. **`Wav2Vec2ClassificationHead` Class**:\n",
    "   - This class defines the classification head that will be added on top of the pre-trained Wav2Vec2 model:\n",
    "     - `dense`: A fully connected layer with the same size as the hidden states of the model.\n",
    "     - `dropout`: A dropout layer to prevent overfitting, with a dropout rate taken from the model configuration.\n",
    "     - `out_proj`: A linear layer projecting the hidden states to the final classification logits, with the number of output units corresponding to the number of labels.\n",
    "\n",
    "2. **`Wav2Vec2ForSpeechClassification` Class**:\n",
    "   - This class extends `Wav2Vec2PreTrainedModel` and combines the pre-trained Wav2Vec2 model with the `Wav2Vec2ClassificationHead`:\n",
    "     - `self.wav2vec2`: The pre-trained Wav2Vec2 model that processes the audio input.\n",
    "     - `self.classifier`: The classification head added on top of the Wav2Vec2 model.\n",
    "     - `self.freeze_feature_extractor()`: This method freezes the feature extractor's parameters to avoid updating them during training.\n",
    "\n",
    "3. **Pooling Strategy (`merged_strategy`)**:\n",
    "   - This function aggregates the hidden states from the model's outputs using the specified pooling method (`mean`, `sum`, or `max`):\n",
    "     - **\"mean\"**: Averages the hidden states along the sequence dimension.\n",
    "     - **\"sum\"**: Sums the hidden states along the sequence dimension.\n",
    "     - **\"max\"**: Takes the maximum hidden state along the sequence dimension.\n",
    "\n",
    "4. **Forward Method**:\n",
    "   - The `forward()` method defines how the model processes inputs:\n",
    "     - It first passes the input values through the Wav2Vec2 model to get the hidden states.\n",
    "     - Then, it applies the selected pooling strategy (`mean`, `sum`, or `max`) to these hidden states.\n",
    "     - The pooled features are passed through the classification head to produce logits.\n",
    "     - If labels are provided, the loss is computed using `CrossEntropyLoss`.\n",
    "\n",
    "5. **Output**:\n",
    "   - If `return_dict=False`, the method returns the logits (and optionally the loss).\n",
    "   - If `return_dict=True`, it returns a `SpeechClassifierOutput` containing the loss, logits, hidden states, and attentions, providing a detailed output for further analysis.\n",
    "\n",
    "This model architecture is designed to fine-tune Wav2Vec2 for speech classification tasks, integrating the pre-trained speech processing capabilities with a custom classification head and loss computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:23:41.454643Z",
     "iopub.status.busy": "2024-04-27T07:23:41.454308Z",
     "iopub.status.idle": "2024-04-27T07:23:43.308675Z",
     "shell.execute_reply": "2024-04-27T07:23:43.307855Z",
     "shell.execute_reply.started": "2024-04-27T07:23:41.454609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Data Collator for CTC with Padding\n",
    "\n",
    "1. **`DataCollatorCTCWithPadding` Class**:\n",
    "   - This class defines a custom data collator for preparing batches of audio data and labels for training, specifically for Connectionist Temporal Classification (CTC) tasks.\n",
    "   - The collator handles padding of the input sequences and labels to ensure consistent batch sizes.\n",
    "\n",
    "2. **Attributes**:\n",
    "   - `feature_extractor`: The Wav2Vec2 feature extractor, used for padding the input sequences.\n",
    "   - `padding`: Controls whether padding should be applied (`True`, `False`, or a string like `'max_length'`).\n",
    "   - `max_length`: The maximum length for padding input sequences.\n",
    "   - `max_length_labels`: The maximum length for padding the labels.\n",
    "   - `pad_to_multiple_of`: Optional parameter to pad sequences to a multiple of a given value.\n",
    "   - `pad_to_multiple_of_labels`: Optional parameter to pad labels to a multiple of a given value.\n",
    "\n",
    "3. **`__call__()` Method**:\n",
    "   - This method is invoked when the collator is called during batching:\n",
    "     - `input_features`: Extracts and prepares the `input_values` from each feature in the dataset.\n",
    "     - `label_features`: Collects the label data from the features.\n",
    "     - `d_type`: Determines the data type for the labels (`torch.long` for integer labels or `torch.float` for continuous labels).\n",
    "     - `batch`: Uses the `feature_extractor` to pad the `input_values` to the desired length and format.\n",
    "     - The labels are converted into a PyTorch tensor and added to the batch.\n",
    "\n",
    "4. **Output**:\n",
    "   - The method returns a dictionary containing:\n",
    "     - `input_values`: The padded input audio features.\n",
    "     - `labels`: The padded label tensor.\n",
    "\n",
    "This data collator ensures that audio inputs and their corresponding labels are properly padded, making them suitable for training with models that require fixed-length input sequences, such as the Wav2Vec2-based model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:23:43.310538Z",
     "iopub.status.busy": "2024-04-27T07:23:43.310121Z",
     "iopub.status.idle": "2024-04-27T07:23:43.321780Z",
     "shell.execute_reply": "2024-04-27T07:23:43.320986Z",
     "shell.execute_reply.started": "2024-04-27T07:23:43.310501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Data Collator Instance\n",
    "\n",
    "1. **`DataCollatorCTCWithPadding` Instance**:\n",
    "   - `data_collator = DataCollatorCTCWithPadding(feature_extractor=feature_extractor, padding=True)` creates an instance of the `DataCollatorCTCWithPadding` class.\n",
    "   \n",
    "2. **Parameters**:\n",
    "   - `feature_extractor`: The `Wav2Vec2FeatureExtractor` is passed to the data collator, enabling it to pad input sequences accordingly.\n",
    "   - `padding=True`: This ensures that padding is applied to the input sequences. The padding will be done automatically based on the maximum sequence length in the batch or according to other specified parameters.\n",
    "\n",
    "This `data_collator` is then used during training to prepare batches of input data, handling padding and ensuring that each batch has a consistent size, making it ready for processing by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:23:43.323153Z",
     "iopub.status.busy": "2024-04-27T07:23:43.322891Z",
     "iopub.status.idle": "2024-04-27T07:23:43.336032Z",
     "shell.execute_reply": "2024-04-27T07:23:43.335161Z",
     "shell.execute_reply.started": "2024-04-27T07:23:43.323124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(feature_extractor=feature_extractor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Metric Computation Function\n",
    "\n",
    "1. **`compute_metrics` Function**:\n",
    "   - This function calculates the accuracy metric for model evaluation.\n",
    "   - It is designed to be used with the Hugging Face `Trainer` API to evaluate the model's performance on the validation or test dataset.\n",
    "\n",
    "2. **Parameters**:\n",
    "   - `p: EvalPrediction`: The function takes an `EvalPrediction` object, which contains the model's predictions and the true labels.\n",
    "\n",
    "3. **Steps**:\n",
    "   - `preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions`: This line extracts the predictions. If the predictions are in a tuple (e.g., logits and hidden states), it selects the first element, which is the logits.\n",
    "   - `preds = np.argmax(preds, axis=1)`: This applies the `argmax` function along the axis of the logits to get the predicted class labels (the class with the highest probability).\n",
    "   - `accuracy = (preds == p.label_ids).astype(np.float32).mean().item()`: This compares the predicted labels with the true labels (`p.label_ids`), calculates the accuracy, and converts it into a scalar value using `.item()`.\n",
    "\n",
    "4. **Output**:\n",
    "   - The function returns a dictionary containing the calculated accuracy value:\n",
    "     - `{\"accuracy\": accuracy}`\n",
    "\n",
    "This function is used during model evaluation to compute the accuracy of predictions, providing a key metric for model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T07:23:43.337970Z",
     "iopub.status.busy": "2024-04-27T07:23:43.337233Z",
     "iopub.status.idle": "2024-04-27T07:23:43.349321Z",
     "shell.execute_reply": "2024-04-27T07:23:43.348573Z",
     "shell.execute_reply.started": "2024-04-27T07:23:43.337939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Pre-trained Wav2Vec2 Model for Speech Classification\n",
    "\n",
    "1. **`Wav2Vec2ForSpeechClassification.from_pretrained()`**:\n",
    "   - This method is used to load a pre-trained Wav2Vec2 model with a custom speech classification head.\n",
    "   \n",
    "2. **Parameters**:\n",
    "   - `model_name_or_path`: Specifies the name or path of the pre-trained model to load. In this case, it points to `\"facebook/wav2vec2-base-100k-voxpopuli\"`, a Wav2Vec2 model trained on a large corpus of spoken language.\n",
    "   - `config`: The configuration object that defines model settings, such as the number of labels and pooling mode. This configuration is passed to ensure the model is correctly adapted to your task.\n",
    "\n",
    "3. **Result**:\n",
    "   - The method returns a model that consists of the Wav2Vec2 feature extractor and a custom classification head for speech classification. This model is now ready for fine-tuning on your specific dataset.\n",
    "\n",
    "By calling this method, you initialize a pre-trained Wav2Vec2 model, which can now be used for further training, fine-tuning, or evaluation on the speech classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c481711a2df4f56a6e4e8cd944183a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-100k-voxpopuli and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Feature Extractor\n",
    "\n",
    "1. **`model.freeze_feature_extractor()`**:\n",
    "   - This method freezes the parameters of the Wav2Vec2 feature extractor, preventing it from being updated during training.\n",
    "   - Freezing the feature extractor is commonly done in transfer learning when you want to fine-tune only the classification head of the model, while keeping the pre-trained weights of the feature extractor fixed.\n",
    "   \n",
    "2. **Purpose**:\n",
    "   - Freezing the feature extractor helps to retain the learned features from the pre-trained model (such as the low-level audio features) and prevents overfitting when there is limited labeled data.\n",
    "   - This step reduces the computational load, as the feature extractor does not require gradient calculations, speeding up training.\n",
    "\n",
    "3. **Result**:\n",
    "   - After this method is called, only the parameters of the classification head will be updated during training, which helps focus learning on the specific classification task without affecting the pre-trained audio features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Training Arguments\n",
    "\n",
    "1. **`TrainingArguments`**:\n",
    "   - This class is used to define the training configuration for the Hugging Face `Trainer` API, specifying the parameters for model training and evaluation.\n",
    "\n",
    "2. **Parameters**:\n",
    "   - `output_dir`: The directory where the model checkpoints and logs will be saved. In this case, it is set to `\"/kaggle/working\"`.\n",
    "   - `per_device_train_batch_size`: The batch size used during training on each device (GPU or CPU). Set to `4` here.\n",
    "   - `per_device_eval_batch_size`: The batch size used during evaluation on each device. Also set to `4`.\n",
    "   - `gradient_accumulation_steps`: Number of steps to accumulate gradients before updating the model parameters. This effectively increases the batch size by accumulating gradients over multiple steps, and it's set to `2` here.\n",
    "   - `evaluation_strategy`: Defines when to run evaluation during training. Set to `\"steps\"`, meaning evaluation will occur every few steps as specified by `eval_steps`.\n",
    "   - `num_train_epochs`: The number of training epochs. Here, it is set to `1.0` (just one epoch of training).\n",
    "   - `fp16`: A flag indicating whether to use 16-bit floating-point precision (mixed precision training), which can speed up training and reduce memory usage. Set to `True`.\n",
    "   - `save_steps`: Defines the frequency (in terms of steps) to save model checkpoints. Here, it is set to every `10` steps.\n",
    "   - `eval_steps`: Defines the frequency (in terms of steps) to run evaluation during training. Set to `10` steps.\n",
    "   - `logging_steps`: Defines the frequency (in terms of steps) to log training information. Here, it is set to `10` steps.\n",
    "   - `learning_rate`: The learning rate for the optimizer. Set to `1e-4` (0.0001).\n",
    "   - `save_total_limit`: The maximum number of model checkpoints to keep. Old checkpoints will be deleted to ensure that only the last two are saved during training.\n",
    "   \n",
    "3. **Result**:\n",
    "   - These settings help control how the training process will proceed, including how often to save and evaluate the model, the batch size, and the learning rate. The arguments are passed to the `Trainer` to train and evaluate the model effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1.0,\n",
    "    fp16=True,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the Model Save Location\n",
    "\n",
    "1. **`print(\"Model saved at:\", training_args.output_dir)`**:\n",
    "   - This line outputs the directory where the trained model and its checkpoints will be saved during the training process.\n",
    "   \n",
    "2. **Purpose**:\n",
    "   - It ensures that the user knows where to find the saved models after training. This information is useful when tracking the output of the model or when further processing the results.\n",
    "   \n",
    "3. **Result**:\n",
    "   - The output will be the path to the directory specified by `output_dir` in the `TrainingArguments`. In this case, it will print:\n",
    "     - `\"Model saved at: /kaggle/working\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: /kaggle/working\n"
     ]
    }
   ],
   "source": [
    "print(\"Model saved at:\", training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Trainer Class: CTCTrainer\n",
    "\n",
    "1. **`CTCTrainer`**:\n",
    "   - This class extends the `Trainer` class from Hugging Face and overrides the `training_step` method to implement custom training behavior, especially for models with a Connectionist Temporal Classification (CTC) loss.\n",
    "\n",
    "2. **Parameters**:\n",
    "   - The `training_step` method is called during each training iteration to compute and backpropagate the loss.\n",
    "\n",
    "3. **Key Modifications**:\n",
    "   - **Input Preparation**: `inputs = self._prepare_inputs(inputs)` ensures that inputs are properly formatted for the model.\n",
    "   - **Automatic Mixed Precision (AMP)**: If AMP is enabled, the loss is calculated with `autocast()` to perform mixed-precision training, reducing memory usage and speeding up training. The loss is then scaled back with `self.scaler.scale(loss).backward()`.\n",
    "   - **Gradient Accumulation**: If `gradient_accumulation_steps > 1`, the loss is divided by the number of accumulation steps. This helps simulate larger batch sizes without increasing memory usage.\n",
    "   - **Backpropagation**: Depending on the training setup (AMP, Apex, DeepSpeed, or standard PyTorch), the loss is backpropagated using the appropriate method to avoid overflow or instability during training.\n",
    "\n",
    "4. **Purpose**:\n",
    "   - The custom `CTCTrainer` class ensures that the model is trained using the specified loss function while supporting advanced training techniques like gradient accumulation and automatic mixed precision.\n",
    "   \n",
    "5. **Result**:\n",
    "   - The method returns the loss after backpropagation, which is detached from the computation graph to avoid unnecessary tracking of gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        if self.use_amp:\n",
    "            with autocast():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "        else:\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Trainer\n",
    "\n",
    "1. **`Trainer`**:\n",
    "   - The `Trainer` class from Hugging Face is used to handle the training and evaluation process in a more efficient way, abstracting much of the complexity involved in model training.\n",
    "   \n",
    "2. **Parameters**:\n",
    "   - `model`: The model to be trained, in this case, a custom `Wav2Vec2ForSpeechClassification` model.\n",
    "   - `data_collator`: The data collator used to process and pad the inputs to the model, defined earlier as `DataCollatorCTCWithPadding`.\n",
    "   - `args`: The training arguments, passed from the `TrainingArguments` object that defines how the training will proceed (batch size, evaluation strategy, etc.).\n",
    "   - `compute_metrics`: A function to compute evaluation metrics (e.g., accuracy) after each evaluation phase, which uses the `compute_metrics` function defined earlier.\n",
    "   - `train_dataset`: The training dataset, which contains the training examples (audio files and labels).\n",
    "   - `eval_dataset`: The evaluation dataset, containing the validation examples.\n",
    "   - `tokenizer`: The tokenizer or feature extractor used to preprocess the input data, in this case, `Wav2Vec2FeatureExtractor`, which handles the preprocessing of audio data.\n",
    "\n",
    "3. **Purpose**:\n",
    "   - The `Trainer` object simplifies the process of training a model, handling many of the common steps like forward/backward passes, logging, and evaluation. It uses the provided datasets, model, and arguments to run training and evaluation loops.\n",
    "   \n",
    "4. **Result**:\n",
    "   - The `trainer` object is now ready to perform training and evaluation using the provided datasets and configurations. The model will be trained based on the parameters defined in the `TrainingArguments`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240427_075352-fvn7d50l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/krisha-darji/huggingface/runs/fvn7d50l' target=\"_blank\">graceful-music-15</a></strong> to <a href='https://wandb.ai/krisha-darji/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/krisha-darji/huggingface' target=\"_blank\">https://wandb.ai/krisha-darji/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/krisha-darji/huggingface/runs/fvn7d50l' target=\"_blank\">https://wandb.ai/krisha-darji/huggingface/runs/fvn7d50l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 5:29:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.078400</td>\n",
       "      <td>2.101141</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.082800</td>\n",
       "      <td>2.108113</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.063800</td>\n",
       "      <td>2.119702</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.044900</td>\n",
       "      <td>2.111104</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.104300</td>\n",
       "      <td>2.097295</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.069700</td>\n",
       "      <td>2.090898</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.060600</td>\n",
       "      <td>2.089742</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.104900</td>\n",
       "      <td>2.093284</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.129000</td>\n",
       "      <td>2.087180</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.088300</td>\n",
       "      <td>2.085870</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.082500</td>\n",
       "      <td>2.085440</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.078100</td>\n",
       "      <td>2.084805</td>\n",
       "      <td>0.128125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.052600</td>\n",
       "      <td>2.086683</td>\n",
       "      <td>0.124063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.117600</td>\n",
       "      <td>2.086537</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.068200</td>\n",
       "      <td>2.085906</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.095100</td>\n",
       "      <td>2.086388</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.098100</td>\n",
       "      <td>2.089615</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.109800</td>\n",
       "      <td>2.084961</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.078600</td>\n",
       "      <td>2.082879</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.081000</td>\n",
       "      <td>2.083216</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.099600</td>\n",
       "      <td>2.082684</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.129000</td>\n",
       "      <td>2.081829</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.078700</td>\n",
       "      <td>2.081831</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.100200</td>\n",
       "      <td>2.083294</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.078600</td>\n",
       "      <td>2.084998</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.063200</td>\n",
       "      <td>2.085225</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.077600</td>\n",
       "      <td>2.088798</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.093600</td>\n",
       "      <td>2.088969</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.119700</td>\n",
       "      <td>2.086356</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.082900</td>\n",
       "      <td>2.085263</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.082100</td>\n",
       "      <td>2.085628</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.113100</td>\n",
       "      <td>2.084872</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.086900</td>\n",
       "      <td>2.083975</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.085000</td>\n",
       "      <td>2.084089</td>\n",
       "      <td>0.126875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.089500</td>\n",
       "      <td>2.083526</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.096400</td>\n",
       "      <td>2.083086</td>\n",
       "      <td>0.124063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.100600</td>\n",
       "      <td>2.083225</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.093900</td>\n",
       "      <td>2.082955</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.082400</td>\n",
       "      <td>2.082712</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.097300</td>\n",
       "      <td>2.082441</td>\n",
       "      <td>0.116563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=2.0884689331054687, metrics={'train_runtime': 21568.909, 'train_samples_per_second': 0.148, 'train_steps_per_second': 0.019, 'total_flos': 4.2624103378027565e+17, 'train_loss': 2.0884689331054687, 'epoch': 1.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/trained_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "\n",
    "1. **`trainer.save_model(output_dir)`**:\n",
    "   - This method saves the trained model to the specified directory after training or during any point in the training process.\n",
    "   \n",
    "2. **Parameters**:\n",
    "   - `output_dir`: The directory where the model will be saved. This directory is specified in the `TrainingArguments` (in this case, it is `\"/kaggle/working\"`).\n",
    "   \n",
    "3. **Purpose**:\n",
    "   - The purpose of this line is to persist the trained model so that it can be reused later for inference or fine-tuning. This step is crucial for saving the model's weights and configurations after training.\n",
    "   \n",
    "4. **Result**:\n",
    "   - The trained model will be saved to the specified directory, and this model can later be loaded using `from_pretrained` for further use, such as making predictions or fine-tuning on other datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Wav2Vec2ForSpeechClassification"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type(model)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4826429,
     "sourceId": 8158965,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
